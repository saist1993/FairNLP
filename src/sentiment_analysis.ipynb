{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "import torchtext.experimental\n",
    "import torchtext.experimental.vectors\n",
    "from torchtext.experimental.datasets.raw.text_classification import RawTextIterableDataset\n",
    "from torchtext.experimental.datasets.text_classification import TextClassificationDataset\n",
    "from torchtext.experimental.functional import sequential_transforms, vocab_func, totensor\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "def transform_dataframe_to_dict(data_frame, spacy_model=\"en_core_web_sm\", max_length=None):\n",
    "    final_data = []\n",
    "    for i in data_frame.iterrows():\n",
    "        temp = {\n",
    "            'original_text': i[1]['comment'],\n",
    "            'lable': i[1]['is_toxic'],\n",
    "            'toxicity': i[1]['toxicity'],\n",
    "            'text': clean_text(i[1]['comment'])\n",
    "        }\n",
    "        final_data.append(temp)\n",
    "    texts = [f['text'] for f in final_data]\n",
    "    tokenized_text = batch_tokenize(texts=texts, spacy_model=spacy_model, max_length=max_length)\n",
    "    for index, f in enumerate(final_data):\n",
    "        f['tokenized_text'] = tokenized_text[index]\n",
    "        final_data[index] = f\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "def clean_text(text:str):\n",
    "    \"\"\"\n",
    "    cleans text casing puntations and special characters. Removes extra space\n",
    "    \"\"\"\n",
    "    text = re.sub('[^ a-zA-Z0-9]|unk', '', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Cleans the data and tokenizes it\"\"\"\n",
    "\n",
    "    def __init__(self, spacy_model:str=\"en_core_web_sm\", clean_text=clean_text, max_length=None):\n",
    "        self.tokenizer_model = spacy.load(\"en_core_web_sm\")\n",
    "        self.clean_text = clean_text\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def tokenize(self, s):\n",
    "        if self.clean_text:\n",
    "            s = clean_text(s)\n",
    "        doc = self.tokenizer_model(s)\n",
    "        tokens = [token.text for token in doc]\n",
    "        \n",
    "        if self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "def batch_tokenize(texts:list, spacy_model=\"en_core_web_sm\", max_length=None):\n",
    "    \"\"\"tokenizes a list via nlp pipeline space\"\"\"\n",
    "    nlp = spacy.load(spacy_model)\n",
    "    \n",
    "    tokenized_list = []\n",
    "    \n",
    "    if max_length:\n",
    "        for doc in tqdm(nlp.pipe(texts, disable=[\"ner\", \"tok2vec\"])):\n",
    "            tokenized_list.append([t.text for t in doc][:max_length])\n",
    "    else:\n",
    "        for doc in tqdm(nlp.pipe(texts, disable=[\"ner\", \"tok2vec\"])):\n",
    "            tokenized_list.append([t.text for t in doc])\n",
    "    \n",
    "    return tokenized_list\n",
    "    \n",
    "tokenizer = Tokenizer(spacy_model=\"en_core_web_sm\", clean_text=clean_text, max_length=-1)\n",
    "print(tokenizer.tokenize(\"This is a test sentence ?? \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5f35a8a7bc42e8b3ef5f32380fb789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ab58b9de9645b7ba0e8f97b6817264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faec25db656e4952979320ccd4b6dec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# read the csv files and do a process over it \n",
    "\n",
    "debias_train = Path('../data/wiki_debias_train.csv')\n",
    "debias_dev = Path('../data/wiki_debias_dev.csv')\n",
    "debias_test = Path('../data/wiki_debias_test.csv')\n",
    "\n",
    "# Optimize this later. We don't need pandas dataframe\n",
    "debias_train_raw = transform_dataframe_to_dict(pd.read_csv(debias_train))\n",
    "debias_dev_raw = transform_dataframe_to_dict(pd.read_csv(debias_dev))\n",
    "debias_test_raw = transform_dataframe_to_dict(pd.read_csv(debias_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_from_data(raw_train_data, raw_dev_data):\n",
    "    \"\"\"This has been made customly for the given dataset. Need to write your own for any other use case\"\"\"\n",
    "    \n",
    "    token_freqs = collections.Counter()\n",
    "    for data_point in raw_train_data:\n",
    "        token_freqs.update(data_point['tokenized_text'])\n",
    "    for data_point in raw_dev_data:\n",
    "        token_freqs.update(data_point['tokenized_text'])\n",
    "#     token_freqs.update(data_point['tokenized_text'] for data_point in raw_train_data)\n",
    "#     token_freqs.update(data_point['tokenized_text'] for data_point in raw_dev_data)    \n",
    "    vocab = torchtext.vocab.Vocab(token_freqs)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(raw_data, vocab):\n",
    "    \"\"\"raw data is assumed to be tokenized\"\"\"\n",
    "    final_data = [(data_point['lable'], data_point['tokenized_text']) for data_point in raw_data]\n",
    "    text_transformation = sequential_transforms(vocab_func(vocab),\n",
    "                                               totensor(dtype=torch.long))\n",
    "    label_transform = sequential_transforms(totensor(dtype=torch.long))\n",
    "    \n",
    "    transforms = (label_transform, text_transformation)\n",
    "    \n",
    "    return TextClassificationDataset(final_data,vocab,transforms)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_data(raw_train_data=debias_train_raw,raw_dev_data=debias_dev_raw )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = process_data(raw_data=debias_train_raw, vocab=vocab)\n",
    "dev_data = process_data(raw_data=debias_dev_raw, vocab=vocab)\n",
    "test_data = process_data(raw_data=debias_test_raw, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def collate(self, batch):\n",
    "        labels, text = zip(*batch)\n",
    "        labels = torch.LongTensor(labels)\n",
    "        lengths = torch.LongTensor([len(x) for x in text])\n",
    "        text = nn.utils.rnn.pad_sequence(text, padding_value= self.pad_idx)\n",
    "        \n",
    "        return labels, text, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = '<pad>'\n",
    "pad_idx = vocab[pad_token]\n",
    "\n",
    "collator = Collator(pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_iterator = torch.utils.data.DataLoader(train_data, \n",
    "                                             batch_size, \n",
    "                                             shuffle = True, \n",
    "                                             collate_fn = collator.collate)\n",
    "\n",
    "dev_iterator = torch.utils.data.DataLoader(dev_data, \n",
    "                                             batch_size, \n",
    "                                             shuffle = False, \n",
    "                                             collate_fn = collator.collate)\n",
    "\n",
    "test_iterator = torch.utils.data.DataLoader(test_data, \n",
    "                                            batch_size, \n",
    "                                            shuffle = False, \n",
    "                                            collate_fn = collator.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, n_layers, dropout, pad_idx):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx = pad_idx)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers = n_layers, bidirectional = True, dropout = dropout)\n",
    "        self.fc = nn.Linear(2 * hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "\n",
    "        # text = [seq len, batch size]\n",
    "        # lengths = [batch size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "\n",
    "        # embedded = [seq len, batch size, emb dim]\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths, enforce_sorted = False)\n",
    "\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        # outputs = [seq_len, batch size, n directions * hid dim]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        hidden_fwd = hidden[-2]\n",
    "        hidden_bck = hidden[-1]\n",
    "\n",
    "        # hidden_fwd/bck = [batch size, hid dim]\n",
    "\n",
    "        hidden = torch.cat((hidden_fwd, hidden_bck), dim = 1)\n",
    "\n",
    "        # hidden = [batch size, hid dim * 2]\n",
    "\n",
    "        prediction = self.fc(self.dropout(hidden))\n",
    "\n",
    "        # prediction = [batch size, output dim]\n",
    "\n",
    "        return prediction\n",
    "    \n",
    "def initialize_parameters(m):\n",
    "    if isinstance(m, nn.Embedding):\n",
    "        nn.init.uniform_(m.weight, -0.05, 0.05)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for n, p in m.named_parameters():\n",
    "            if 'weight_ih' in n:\n",
    "                i, f, g, o = p.chunk(4)\n",
    "                nn.init.xavier_uniform_(i)\n",
    "                nn.init.xavier_uniform_(f)\n",
    "                nn.init.xavier_uniform_(g)\n",
    "                nn.init.xavier_uniform_(o)\n",
    "            elif 'weight_hh' in n:\n",
    "                i, f, g, o = p.chunk(4)\n",
    "                nn.init.orthogonal_(i)\n",
    "                nn.init.orthogonal_(f)\n",
    "                nn.init.orthogonal_(g)\n",
    "                nn.init.orthogonal_(o)\n",
    "            elif 'bias' in n:\n",
    "                i, f, g, o = p.chunk(4)\n",
    "                nn.init.zeros_(i)\n",
    "                nn.init.ones_(f)\n",
    "                nn.init.zeros_(g)\n",
    "                nn.init.zeros_(o)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (embedding): Embedding(252723, 300, padding_idx=1)\n",
       "  (lstm): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_dim = len(vocab)\n",
    "emb_dim = 300\n",
    "hid_dim = 256\n",
    "output_dim = 2\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "model = BiLSTM(input_dim, emb_dim, hid_dim, output_dim, n_layers, dropout, pad_idx)\n",
    "model.apply(initialize_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading new vector file\n"
     ]
    }
   ],
   "source": [
    "# set embeddings to your liking\n",
    "# def get_pretrained_embedding(initial_embedding, pretrained_vocab, pretrained_vectors, vocab, unk_token):\n",
    "    \n",
    "#     pretrained_embedding = torch.FloatTensor(initial_embedding.weight.clone()).detach()    \n",
    "    \n",
    "#     unk_tokens = []\n",
    "    \n",
    "#     for idx, token in tqdm(enumerate(vocab.itos)):\n",
    "#         if token in pretrained_vocab:\n",
    "#             pretrained_vector = torch.tensor(pretrained_vectors[token],device=device)\n",
    "#             pretrained_embedding[idx] = pretrained_vector\n",
    "#         else:\n",
    "#             unk_tokens.append(token)\n",
    "        \n",
    "#     return pretrained_embedding, unk_tokens\n",
    "\n",
    "\n",
    "# set embeddings to your liking\n",
    "\n",
    "\n",
    "print(\"reading new vector file\")\n",
    "pretrained_embedding = gensim.models.KeyedVectors.load_word2vec_format(\"../../bias-in-nlp/src/testvec1\")\n",
    "pretrained_vocab = [key for key in pretrained_embedding.vocab.keys()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_embedding(initial_embedding, pretrained_vocab, pretrained_vectors, vocab, unk_token,device):\n",
    "    \n",
    "    pretrained_embedding = torch.FloatTensor(initial_embedding.weight.clone()).cpu().detach().numpy()  \n",
    "    \n",
    "    unk_tokens = []\n",
    "    \n",
    "    for idx, token in tqdm(enumerate(vocab.itos)):\n",
    "        try:\n",
    "            pretrained_embedding[idx] = pretrained_vectors[token]\n",
    "        except KeyError:\n",
    "            unk_tokens.append(token)\n",
    "    \n",
    "    pretrained_embedding = torch.from_numpy(pretrained_embedding).to(device)\n",
    "    return pretrained_embedding, unk_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420b50f5fe3243d999e94738fc96eb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0005,  0.0132, -0.0177,  ..., -0.0187, -0.0095,  0.0047],\n",
       "        [-0.0227,  0.0025,  0.0059,  ..., -0.0050, -0.0287,  0.0113],\n",
       "        [ 0.0543,  0.0044,  0.0116,  ...,  0.0142, -0.0106, -0.0004],\n",
       "        ...,\n",
       "        [-0.0045, -0.0004, -0.0314,  ...,  0.0352,  0.0335, -0.0246],\n",
       "        [-0.0066,  0.0040, -0.0320,  ..., -0.0102, -0.0273,  0.0151],\n",
       "        [-0.0180,  0.0105, -0.0027,  ...,  0.0232,  0.0074,  0.0356]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"updating embeddings\")\n",
    "unk_token = '<unk>'\n",
    "pretrained_embedding, unk_tokens = get_pretrained_embedding(initial_embedding=model.embedding, \n",
    "                                                            pretrained_vocab=pretrained_vocab,\n",
    "                                                            pretrained_vectors=pretrained_embedding,\n",
    "                                                            vocab=vocab, \n",
    "                                                            unk_token=unk_token,\n",
    "                                                           device=device)\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, labels):\n",
    "    top_predictions = predictions.argmax(1, keepdim = True)\n",
    "    correct = top_predictions.eq(labels.view_as(top_predictions)).sum()\n",
    "    accuracy = correct.float() / labels.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for labels, text, lengths in iterator:\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        text = text.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(text, lengths)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        acc = calculate_accuracy(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for labels, text, lengths in iterator:\n",
    "\n",
    "            labels = labels.to(device)\n",
    "            text = text.to(device)\n",
    "            \n",
    "            predictions = model(text, lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            acc = calculate_accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
    "    \n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'bilstm-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec =  KeyedVectors.load_word2vec_format(\"../../bias-in-nlp/src/testvec1\", binary=True)\n",
    "\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(os.path.dirname(__file__), 'GoogleNews-vectors-negative300.bin'), binary=True)\n",
    "\n",
    "pretrained_embedding = gensim.models.KeyedVectors.load_word2vec_format(\"../../bias-in-nlp/src/testvec1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vocab = [key for key in pretrained_embedding.vocab.keys()]\n",
    "pretrained_vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_pretrained_embedding(initial_embedding, pretrained_vocab, pretrained_vectors, vocab, unk_token):\n",
    "    \n",
    "    pretrained_embedding = torch.FloatTensor(initial_embedding.weight.clone()).detach()    \n",
    "    \n",
    "    unk_tokens = []\n",
    "    \n",
    "    for idx, token in tqdm(enumerate(vocab.itos)):\n",
    "        if token in pretrained_vocab:\n",
    "            pretrained_vector = torch.tensor(pretrained_vectors[token],device=device)\n",
    "            pretrained_embedding[idx] = pretrained_vector\n",
    "        else:\n",
    "            unk_tokens.append(token)\n",
    "        \n",
    "    return pretrained_embedding, unk_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = '<unk>'\n",
    "pretrained_embedding, unk_tokens = get_pretrained_embedding(initial_embedding=model.embedding, \n",
    "                                                            pretrained_vocab=pretrained_vocab,\n",
    "                                                            pretrained_vectors=pretrained_embedding,\n",
    "                                                            vocab=vocab, \n",
    "                                                            unk_token=unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy(pretrained_embedding.wv.syn0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set embeddings to your liking\n",
    "def get_pretrained_embedding(initial_embedding, pretrained_vocab, pretrained_vectors, vocab, unk_token):\n",
    "    \n",
    "    pretrained_embedding = torch.FloatTensor(initial_embedding.weight.clone()).detach()    \n",
    "    \n",
    "    unk_tokens = []\n",
    "    \n",
    "    for idx, token in tqdm(enumerate(vocab.itos)):\n",
    "        if token in pretrained_vocab:\n",
    "            pretrained_vector = torch.tensor(pretrained_vectors[token],device=device)\n",
    "            pretrained_embedding[idx] = pretrained_vector\n",
    "        else:\n",
    "            unk_tokens.append(token)\n",
    "        \n",
    "    return pretrained_embedding, unk_tokens\n",
    "\n",
    "\n",
    "# set embeddings to your liking\n",
    "def get_pretrained_embedding(initial_embedding, pretrained_vocab, pretrained_vectors, vocab, unk_token,device):\n",
    "    \n",
    "    pretrained_embedding = torch.FloatTensor(initial_embedding.weight.clone()).cpu().detach().numpy()  \n",
    "    \n",
    "    unk_tokens = []\n",
    "    \n",
    "    for idx, token in tqdm(enumerate(vocab.itos)):\n",
    "        try:\n",
    "            pretrained_embedding[idx] = pretrained_vector\n",
    "        except KeyError:\n",
    "            unk_tokens.append(token)\n",
    "    \n",
    "    pretrained_embedding = torch.from_numpy(pretrained_embedding).to(device)\n",
    "    return pretrained_embedding, unk_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
